{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "import os\r\n",
    "import requests\r\n",
    "import json\r\n",
    "import datetime\r\n",
    "import shutil\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "from random import choice\r\n",
    "from selenium.common.exceptions import TimeoutException\r\n",
    "import time\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.support.ui import WebDriverWait\r\n",
    "from selenium.webdriver.common.by import By\r\n",
    "from selenium.webdriver.support import expected_conditions as EC\r\n",
    "from urllib.parse import urlparse\r\n",
    "from time import sleep, time\r\n",
    "from random import uniform, randint\r\n",
    "import json\r\n",
    "from urllib.parse import urlsplit\r\n",
    "import urllib3\r\n",
    "from glob import glob \r\n",
    "import wget \r\n",
    "import requests\r\n",
    "from pathlib import Path\r\n",
    "import io\r\n",
    "import boto3\r\n",
    "\r\n",
    "# function to convert json to csv\r\n",
    "\r\n",
    "def json_to_csv(json, filepath, lines):\r\n",
    "    \r\n",
    "    df = pd.read_json(json, lines=lines)\r\n",
    "\r\n",
    "    df.to_csv(filepath, index=False)\r\n",
    "\r\n",
    "\r\n",
    "def list_duplicates(seq):\r\n",
    "\r\n",
    "    \"\"\"List all of the duplicate file names in the download path d\"\"\"\r\n",
    "\r\n",
    "    seen = set()\r\n",
    "    seen_add = seen.add\r\n",
    "    # adds all elements it doesn't know yet to seen and all other to seen_twice\r\n",
    "    seen_twice = set( x for x in seq if x in seen or seen_add(x) )\r\n",
    "    # # turn the set into a list (as requested)\r\n",
    "    return list(seen_twice)\r\n",
    "\r\n",
    "\r\n",
    "# a function to write a get from urllib requests.context to an S3 bucket\r\n",
    "\r\n",
    "\r\n",
    "def write_to_s3(bucket_name, filepath, response):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Write file to S3 bucket\r\n",
    "\r\n",
    "    Args\r\n",
    "    ----\r\n",
    "    filepath : str\r\n",
    "        The path to the file to be uploaded.\r\n",
    "    bucket_name : str\r\n",
    "        The name of the S3 bucket.\r\n",
    "    s3_key : str\r\n",
    "        The key to be used for the file in the S3 bucket.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # create a session and connect to S3\r\n",
    "\r\n",
    "    session = boto3.Session(\r\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\r\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\r\n",
    "    )\r\n",
    "\r\n",
    "    s3 = session.resource('s3')\r\n",
    "\r\n",
    "    # open file and upload to S3\r\n",
    "\r\n",
    "    s3.Bucket(bucket_name).put_object(Key=filepath, Body=response.content)\r\n",
    "\r\n",
    "    # remove the file from the local directory\r\n",
    "\r\n",
    "    # os.remove(filepath)\r\n",
    "\r\n",
    "\r\n",
    "def create_directory(directory):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Create a directory if it doesn't exist.\r\n",
    "\r\n",
    "    Args\r\n",
    "    ----\r\n",
    "    directory : str\r\n",
    "        The path to the directory.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    if not os.path.exists(directory):\r\n",
    "        os.makedirs(directory)\r\n",
    "\r\n",
    "def create_driver(download_path, driver_path):\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Create selenium chrome browser so we can pull the page source passed in url\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # set up Chrome browser for selenium\r\n",
    "\r\n",
    "    options = webdriver.ChromeOptions()\r\n",
    "\r\n",
    "    # add headless option \r\n",
    "\r\n",
    "    options.add_argument(\"headless\")\r\n",
    "\r\n",
    "    # simulate maxing out the browser window\r\n",
    "\r\n",
    "    options.add_argument(\"start-maximized\")\r\n",
    "\r\n",
    "    # remove selenium log level \r\n",
    "\r\n",
    "    options.add_argument(\"--log-level=3\")\r\n",
    "\r\n",
    "    # disable blink features to get around captcha\r\n",
    "\r\n",
    "    options.add_argument(\"--disable-blink-features\")\r\n",
    "\r\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\r\n",
    "\r\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\r\n",
    "\r\n",
    "    # add browser notifications\r\n",
    "\r\n",
    "    options.add_experimental_option(\"prefs\", { \r\n",
    "        \"profile.default_content_setting_values.notifications\": 1 \r\n",
    "    })\r\n",
    "\r\n",
    "    # set download path\r\n",
    "\r\n",
    "    chrome_prefs = {\"download.default_directory\": download_path}\r\n",
    "\r\n",
    "    options.experimental_options[\"prefs\"] = chrome_prefs\r\n",
    "\r\n",
    "    options.add_experimental_option('useAutomationExtension', False)\r\n",
    "\r\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\r\n",
    "\r\n",
    "    # set user agent to avoid being blocked by websites\r\n",
    "\r\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\r\n",
    "\r\n",
    "    driver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.53 Safari/537.36'})\r\n",
    "\r\n",
    "    # get generic headers\r\n",
    "\r\n",
    "    driver.get('https://www.httpbin.org/headers')\r\n",
    "\r\n",
    "    return driver\r\n",
    "\r\n",
    "\r\n",
    "def get_url_data(url, driver=None, is_download=False, is_request=False, wait=False):\r\n",
    "\r\n",
    "    \"\"\"Use driver to get page source or download data\"\"\"\r\n",
    "\r\n",
    "    # create headers for user agent for requests \r\n",
    "\r\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\r\n",
    "\r\n",
    "\r\n",
    "    # if is_download is true, get page or download data\r\n",
    "    if is_download:\r\n",
    "\r\n",
    "        if wait == True:\r\n",
    "            driver.get(url)\r\n",
    "            sleep(10)\r\n",
    "        else:\r\n",
    "            driver.get(url)\r\n",
    "    \r\n",
    "    if is_request:\r\n",
    "\r\n",
    "        response = requests.get(url, headers=headers)\r\n",
    "        return response\r\n",
    "\r\n",
    "        \r\n",
    "    else:\r\n",
    "        try:\r\n",
    "            driver.get(url)\r\n",
    "        except TimeoutException:\r\n",
    "            print(\"Loading took too much time!\")\r\n",
    "        \r\n",
    "\r\n",
    "    return driver\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def download_wait(directory, timeout, nfiles=None):\r\n",
    "    \"\"\"\r\n",
    "    Wait for downloads to finish with a specified timeout.\r\n",
    "\r\n",
    "    Args\r\n",
    "    ----\r\n",
    "    directory : str\r\n",
    "        The path to the folder where the files will be downloaded.\r\n",
    "    timeout : int\r\n",
    "        How many seconds to wait until timing out.\r\n",
    "    nfiles : int, defaults to None\r\n",
    "        If provided, also wait for the expected number of files.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    print(\"Waiting for downloads to finish\")\r\n",
    "\r\n",
    "    seconds = 0\r\n",
    "    dl_wait = True\r\n",
    "    while dl_wait and seconds < timeout:\r\n",
    "        sleep(1)\r\n",
    "        dl_wait = False\r\n",
    "        files = os.listdir(directory)\r\n",
    "        if nfiles and len(files) != nfiles:\r\n",
    "            dl_wait = True\r\n",
    "\r\n",
    "        for fname in files:\r\n",
    "            if fname.endswith('.crdownload'):\r\n",
    "                dl_wait = True\r\n",
    "\r\n",
    "        seconds += 1\r\n",
    "    return seconds\r\n",
    "\r\n",
    "def excel_to_csv(response, filename):\r\n",
    "\r\n",
    "    \"\"\"Convert excel file to csv file\"\"\"\r\n",
    "\r\n",
    "    df = pd.read_excel(response)\r\n",
    "\r\n",
    "    df.to_csv(filename.replace('.xlsx', '.csv'))\r\n",
    "\r\n",
    "\r\n",
    "def get_unc(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Create drivers to bypass captcha for UNC data\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    driver = create_driver(download_path, driver_path)\r\n",
    "\r\n",
    "\r\n",
    "    def _wait_between(a,b):\r\n",
    "        rand=uniform(a, b) \r\n",
    "        sleep(rand)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        \r\n",
    "        try:\r\n",
    "            driver.get(url)\r\n",
    "        except TimeoutException:\r\n",
    "            print(\"Loading took too much time!\")\r\n",
    "        \r\n",
    "        driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/div/div[1]/div[1]/div[2]/div/a[2]').click()\r\n",
    "\r\n",
    "        sleep(10)\r\n",
    "\r\n",
    "        mainWin = driver.current_window_handle  \r\n",
    "\r\n",
    "        # move the driver to the first iFrame \r\n",
    "        driver.switch_to_frame(driver.find_elements_by_tag_name(\"iframe\")[0])\r\n",
    "\r\n",
    "        # *************  locate CheckBox  **************\r\n",
    "        CheckBox = WebDriverWait(driver, 10).until(\r\n",
    "                EC.presence_of_element_located((By.ID ,\"recaptcha-anchor\"))\r\n",
    "                ) \r\n",
    "\r\n",
    "        # *************  click CheckBox  ***************\r\n",
    "        _wait_between(0.5, 0.7)  \r\n",
    "        \r\n",
    "        # making click on captcha CheckBox \r\n",
    "        CheckBox.click()\r\n",
    "\r\n",
    "        # switch back to main window\r\n",
    "\r\n",
    "        driver.switch_to.window(mainWin)\r\n",
    "\r\n",
    "        driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div/div/div/div/div[2]/div/div/div/div/fieldset/div/div/div/div[4]/div/div/div/div/div/span/input').click()\r\n",
    "\r\n",
    "        driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div/div/div/div/div[2]/div/div/div/div/fieldset/div/div/div/a').click()\r\n",
    "\r\n",
    "    return driver\r\n",
    "\r\n",
    "\r\n",
    "def get_duke(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Duke data and download the csv files\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "    \r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    df_cdm = pd.DataFrame()\r\n",
    "\r\n",
    "    df_drg = pd.DataFrame()\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        print(url)\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file \r\n",
    "        df = pd.read_csv(io.StringIO(response.content.decode('utf-8')), error_bad_lines=False)\r\n",
    "        # df = pd.read_csv(response.content)\r\n",
    "        \r\n",
    "        with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "            f.write(response.content)\r\n",
    "\r\n",
    "def get_north_carolina_baptist(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Wake-Forest Baptist data and download the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file \r\n",
    "        with open(os.path.join(download_path, filename.replace('?la=en', '')), 'wb') as f:\r\n",
    "            f.write(response.content)\r\n",
    "        \r\n",
    "\r\n",
    "def get_app(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Applachain Regional Data and download only the CSV data\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        filename = response.headers['Content-Disposition'].strip('\"').replace('inline; filename=\"', '').replace('\"', '')\r\n",
    "\r\n",
    "        # if filename ends with .csv write it out\r\n",
    "\r\n",
    "        if filename.endswith('.csv'):\r\n",
    "            with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "                f.write(response.content)\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "\r\n",
    "\r\n",
    "def get_catawba(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Catawba Valley Regional data and download the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        if filename.endswith('.csv'):\r\n",
    "            with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "                f.write(response.content)\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "\r\n",
    "\r\n",
    "def get_cateret(hospital_id):\r\n",
    "    \r\n",
    "    \"\"\"Get Cateret Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        if filename.endswith('.csv'):\r\n",
    "            with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "                f.write(response.content)\r\n",
    "        else:\r\n",
    "            excel_to_csv(response.content, os.path.join(download_path, filename))\r\n",
    "\r\n",
    "\r\n",
    "def get_cone(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Cone Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        print(filename)\r\n",
    "        if filename.endswith('.csv'):\r\n",
    "            with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "                f.write(response.content)\r\n",
    "        else:\r\n",
    "            continue\r\n",
    "\r\n",
    "def get_first(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get First Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "    meta = ['id' , 'hospital' , 'code' , 'description' , 'codeType' , 'cmsShoppable', 'cranewareShoppable' , \r\n",
    "            'shoppable' , 'level' , 'grossCharge' , 'minAllowable' , 'maxAllowable' , 'avgAllowable' , \r\n",
    "            'nationalPercentile50' , 'nationalPercentile75' , 'nationalPercentile90' , 'totalVol835' , \r\n",
    "            'totalVol837' , 'published' , 'selfPay' , ['name', 'id', 'hospital', 'minAllowable', 'maxAllowable', \r\n",
    "            'avgAllowable', 'exclude']]\r\n",
    "            \r\n",
    "    headers = ['payor.name', 'payor.id', 'payor.hospital', 'payor.minAllowable', 'payor.maxAllowable', 'payor.avgAllowable', \r\n",
    "                'payor.exclude', 'id', 'hospital', 'code', 'description', 'codeType', 'cmsShoppable', 'cranewareShoppable', \r\n",
    "                'shoppable', 'level', 'grossCharge', 'minAllowable', 'maxAllowable', 'avgAllowable', 'nationalPercentile50', \r\n",
    "                'nationalPercentile75', 'nationalPercentile90', 'totalVol835', 'totalVol837', 'published', 'selfPay', \r\n",
    "                'name.id.hospital.minAllowable.maxAllowable.avgAllowable.exclude']\r\n",
    "\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "    \r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        # filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # create pandas dataframe from json \r\n",
    "        # df = pd.read_json()\r\n",
    "        json_data =  json.loads(response.content)['response']\r\n",
    "        df = pd.json_normalize(json_data, record_path='payors', meta=meta, errors='ignore', record_prefix='payor.')\r\n",
    "        filepath = os.path.join(download_path, f'{hospital_id}_standardcharges.csv')\r\n",
    "        # if file does not exist write header \r\n",
    "        if not os.path.isfile(filepath):\r\n",
    "            df.to_csv(filepath, header=headers, index=False)\r\n",
    "        else: # else it exists so append without writing the header\r\n",
    "            df.to_csv(filepath, mode='a', header=False, index=False)\r\n",
    "    # pivot pandas dataframe on codeeType and description\r\n",
    "    # df = pd.read_csv(filepath)\r\n",
    "    # df.pivot(index='codeType', columns='description', values=values).to_csv(os.path.join(download_path, 'pivot.csv'))\r\n",
    "\r\n",
    "        \r\n",
    "def get_iredell(hospital_id):\r\n",
    "\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "    \r\n",
    "    \"\"\"Get Iredell Health data and download only the CSV file\"\"\"\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file \r\n",
    "        with open(os.path.join(download_path, filename.replace('?la=en', '')), 'wb') as f:\r\n",
    "            f.write(response.content)\r\n",
    "    \r\n",
    "\r\n",
    "def get_mission(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Mission Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "    \r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file \r\n",
    "        with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "            f.write(response.content)\r\n",
    "\r\n",
    "def get_nhrmc(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get New Hanover Regional Medical Center data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "         \r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file from excel\r\n",
    "        excel_to_csv(response.content, os.path.join(download_path, filename))\r\n",
    "\r\n",
    "def get_northern(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Northern Regional data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url and replace .json with .csv\r\n",
    "        filename = url.split('/')[-1].replace('.json', '.csv')\r\n",
    "\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "\r\n",
    "        # create pandas dataframe from response\r\n",
    "        df = pd.read_json(response.content, lines=True)\r\n",
    "\r\n",
    "        # drop columns\r\n",
    "        df_all = df.drop(columns=['PACKAGE_TYPE', 'PERCENT_OCCURRENCE_WITHIN_PRIMARY_CODE','SUPPORTING_SERVICE_CODE' ,'SUPPORTING_SERVICE_CODE_DESCRIPTION'])\r\n",
    "\r\n",
    "        # reformat column types\r\n",
    "        for column in df_all:\r\n",
    "            if df_all[column].dtype == 'float64':\r\n",
    "                df_all[column]=pd.to_numeric(df_all[column], downcast='float')\r\n",
    "            if df_all[column].dtype == 'int64':\r\n",
    "                df_all[column]=pd.to_numeric(df_all[column], downcast='integer')\r\n",
    "\r\n",
    "        # write to csv\r\n",
    "        df.to_csv(os.path.join(download_path, filename), index=False)\r\n",
    "\r\n",
    "def get_novant(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Novant Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file \r\n",
    "        with open(os.path.join(download_path, filename), 'wb') as f:\r\n",
    "            f.write(response.content)\r\n",
    "\r\n",
    "def get_vidant(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Vidant Health data and download only the CSV file\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "         \r\n",
    "        # infer filename from url \r\n",
    "        filename = url.split('/')[-1]\r\n",
    "        # download the file\r\n",
    "        print(filename)\r\n",
    "        response = get_url_data(url, is_request=True)\r\n",
    "        # write reponse to csv file from excel\r\n",
    "\r\n",
    "        excel_to_csv(response.content, os.path.join(download_path, filename))\r\n",
    "\r\n",
    "\r\n",
    "def get_atrium(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get Atrium Health data from url\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "\r\n",
    "        filename = url.split('/')[-1].replace('.json', '.csv')\r\n",
    "\r\n",
    "        print(filename)\r\n",
    "\r\n",
    "        # download the file\r\n",
    "\r\n",
    "        http = urllib3.PoolManager()\r\n",
    "        \r\n",
    "        r = http.request('GET', url)\r\n",
    "\r\n",
    "        # create pandas dataframe from response\r\n",
    "        df = pd.read_json(r.data)\r\n",
    "\r\n",
    "        df.to_csv(os.path.join(download_path, filename), index=False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def get_wakemed(hospital_id):\r\n",
    "\r\n",
    "    \"\"\"Get wakemed data from url\"\"\"\r\n",
    "\r\n",
    "    url_list = hospital_urls[hospital_id]\r\n",
    "\r\n",
    "    print(url_list)\r\n",
    "\r\n",
    "    download_path = os.path.join(raw_download_path, hospital_id)\r\n",
    "    \r\n",
    "    create_directory(download_path)\r\n",
    "\r\n",
    "    driver = create_driver(download_path, driver_path)\r\n",
    "\r\n",
    "    for url in url_list:\r\n",
    "    # infee filename from url\r\n",
    "    \r\n",
    "        driver.get(url)\r\n",
    "\r\n",
    "        xpath = '/html/body/app-root/app-allservices/div[1]/div/div[3]/div/app-paginator/div[2]/div/div/button'\r\n",
    "\r\n",
    "        driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xpath))))\r\n",
    "\r\n",
    "        driver.switch_to.alert.accept()\r\n",
    "\r\n",
    "    download_wait(directory=download_path, timeout=150, nfiles=1)\r\n",
    "\r\n",
    "    json_file = os.path.join(download_path, [pos_json for pos_json in os.listdir(download_path)][0])\r\n",
    "\r\n",
    "    wakemed_json = open(json_file, 'r').read()\r\n",
    "\r\n",
    "    wakemed_data = wakemed_json.replace('\\x00', '')\r\n",
    "\r\n",
    "    df = pd.read_json(wakemed_data)\r\n",
    "\r\n",
    "    df.to_csv(os.path.join(download_path, f'{hospital_id}_standardcharges.csv'), index=False)\r\n",
    "\r\n",
    "    os.remove(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "# write a function to itarate through the urls and download the csv data with pandas\r\n",
    "\r\n",
    "abspath = os.path.normpath(os.path.abspath(''))\r\n",
    "\r\n",
    "raw_download_path = os.path.normpath(os.path.join(abspath, 'data', 'raw'))\r\n",
    "\r\n",
    "url_download_path = os.path.normpath(os.path.join(abspath, 'data', 'urls'))\r\n",
    "\r\n",
    "data_urls_path = os.path.join(url_download_path, 'hospital_data_urls.json')\r\n",
    "\r\n",
    "driver_path = r'C:\\Users\\remot\\OneDrive\\Desktop\\Personal\\nc-hospital-transparency\\drivers\\chromedriver.exe'\r\n",
    "\r\n",
    "urls_json = r'C:\\Users\\remot\\OneDrive\\Desktop\\Personal\\nc-hospital-transparency\\data\\urls\\hospital_data_urls.json'\r\n",
    "\r\n",
    "hospital_urls = json.load(open(urls_json))\r\n",
    "\r\n",
    "# get_unc('university-of-north-carolina-hospital')\r\n",
    "\r\n",
    "get_duke('duke-university-hospital')\r\n",
    "\r\n",
    "get_north_carolina_baptist('north-carolina-baptist-hospital')\r\n",
    "\r\n",
    "get_app('app-regional-health-system')\r\n",
    "\r\n",
    "get_catawba('catawba-valley-medical-center')\r\n",
    "\r\n",
    "get_cateret('cateret-health-care')\r\n",
    "\r\n",
    "get_cone('cone-health')\r\n",
    "\r\n",
    "get_first('first-health-moore')\r\n",
    "\r\n",
    "get_first('first-health-montgomery')\r\n",
    "\r\n",
    "get_iredell('iredell-health')\r\n",
    "\r\n",
    "get_mission('mission-health')\r\n",
    "\r\n",
    "get_nhrmc('nhrmc-health')\r\n",
    "\r\n",
    "get_northern('northern-regional')\r\n",
    "\r\n",
    "get_novant('novant-health')\r\n",
    "\r\n",
    "get_wakemed('wakemed-raleigh')\r\n",
    "\r\n",
    "get_wakemed('wakemed-cary')\r\n",
    "\r\n",
    "get_vidant('vidant-health')\r\n",
    "\r\n",
    "get_atrium('atrium-health')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.dukehealth.org//sites/default/files/general_page/56-2070036_DRaH_standardcharges_drg.csv\n",
      "56-2070036_DRaH_standardcharges_drg.csv\n",
      "https://www.dukehealth.org//sites/default/files/general_page/56-2070036_DUH_standardcharges_cdm.csv\n",
      "56-2070036_DUH_standardcharges_cdm.csv\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 12597",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-03f12dc2e463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m# get_unc(university-of-north-carolina-hospital')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mdf_cdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_drg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_duke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duke-university-hospital'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m# get_north_carolina_baptist('north-carolina-baptist-hospital')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-03f12dc2e463>\u001b[0m in \u001b[0;36mget_duke\u001b[1;34m(hospital_id)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_url_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_request\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# write reponse to csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# df = pd.read_csv(response.content)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 12597"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_drg.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 761 entries, 0 to 760\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   MS DRG                 761 non-null    int64 \n",
      " 1   MS DRG w Description   761 non-null    object\n",
      " 2   Medicare Adv - Aetna   761 non-null    object\n",
      " 3   Medicare Adv - BCBS    761 non-null    object\n",
      " 4   Medicare Adv - Humana  761 non-null    object\n",
      " 5   Medicare Adv - United  761 non-null    object\n",
      " 6   Medicare Adv - Other   761 non-null    object\n",
      " 7   Aetna                  761 non-null    object\n",
      " 8   BCBS                   761 non-null    object\n",
      " 9   Cigna                  761 non-null    object\n",
      " 10  Exchange/Ambetter      761 non-null    object\n",
      " 11  Medcost                761 non-null    object\n",
      " 12  United                 761 non-null    object\n",
      " 13  Other Mgd Care         761 non-null    object\n",
      " 14  Tricare                761 non-null    object\n",
      " 15  Workers Comp           761 non-null    object\n",
      " 16  Self Pay               761 non-null    object\n",
      " 17  De-identified Minimum  761 non-null    object\n",
      " 18  De-identified Maximum  761 non-null    object\n",
      "dtypes: int64(1), object(18)\n",
      "memory usage: 113.1+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# ws_access_key_id=os.environ['AWS_ACCESS_KEY_ID']\r\n",
    "\r\n",
    "# ws_access_key_id\r\n",
    "\r\n",
    "aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "aws_secret_access_key"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1bKjkgGBO4YgleMyhhhHr1HgEY6wY33Q/2RA++7T'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d83ad3a39640e45593c9df71430ee3ad7d7609991877e929a0498be1b4dbd18b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}